---
title: "ST443 Group Project - Task 2"
output: html_notebook
editor_options: 
  chunk_output_type: inline
---

# ST443 Group Project

# Task 2: Feature selection

## Introduction

The aim of task 2 is to select properties of a molecule in a compound or random probe determine whether a compound binds to a target site on thrombin. This knowledge is required to design new compounds that can be used in drugs.

# T1.1 Data Preparation and Summary Statistics

```{r}
# Load all libraries required to execute the code in this notebook
library(ggplot2)
library(dplyr)
library(caret)
library(glmnet)
library(pROC)
library(yardstick)
```

```{r}
# Load and view the data
MLData_Task2 <- read.csv("data2.csv.gz", header=TRUE)
View(MLData_Task2)
```

```{r}
# Check for any missing valus in the dataset
any(is.na(MLData_Task2))
```

## Dataset-level statistics

```{r}
# Calculate the Feature-Row ratio
cat("Number of features:", ncol(MLData_Task2), "\n")
cat("Number of samples:", nrow(MLData_Task2), "\n")
cat("Feature-to-sample ratio:", ncol(MLData_Task2) / nrow(MLData_Task2), "\n")
```

As we can see, we are clearly dealing with high-dimensional data, meaning that p \>\> n.

```{r}
# Check if the dataset is balanced, i.e. if the frequency of each class is approx. the same
table(MLData_Task2$label)
```

The table shows that our data is heavily imbalanced - it contains much more observations of class -1 than of class 1. We will take this into account when training our models later.

```{r}
# Calculate the overall sparsity of the dataset (proportion of zeros in the dataset)
sum(MLData_Task2 == 0) / (nrow(MLData_Task2) * ncol(MLData_Task2))
```

The dataset is extremly sparse, i.e. it contains a significant number of zero values relative to its size.

## Feature-level Statistics

```{r}
# Calculate the variance of each feature and check the proportion of low-variance
feature_variances <- apply(MLData_Task2, 2, var)
cat("Proportion of low-variance features (< 0.01):", mean(feature_variances < 0.01), "\n")
```

As we can see, approximately 69% of the features in our dataset have a variance below 0.01. In the upcoming chapters, we will consider the limitations of low-variance features, as they typically provide minimal variability across data points and are less likely to correlate meaningfully with the target variable. Additionally, such features often introduce more noise than signal, reducing their predictive value. To reduce computational complexity and ensure efficient model training on our available hardware, we will apply a variance threshold for some models, selecting only features with variance above a certain level.

# T2.2 Training and Evaluation of Feature Selection methods

For all our models, we will be using an 80/20 Train to Test data split. We will run a cross-validation on the train set to select the best parameter values for our models. Then we will fit our model with the selected parameter values on the test data to see how well it performs on unseen data. The metric of evaluation in all cases is the balanced accuracy. Since balanced accuracy is not available as a pre-defined metric in the R methods we are using, we have implemented a custom function to calculate it.

# Lasso with Linear Regression + Classification

We chose Lasso with linear regression and subsequent conversion at a threshold for classification as our first model. As it's a regularization method that adds a penalty to the model's coefficients and shrinking less important ones to zero, it effectively performs feature selection.

At first we create the training and testing set.

```{r}
# Random split (80% training, 20% testing)
set.seed(123)
train_indices <- sample(1:nrow(MLData_Task2), size = 0.8 * nrow(MLData_Task2))
```

```{r}
# Create training and testing datasets
train2_data <- MLData_Task2[train_indices, ]
test2_data <- MLData_Task2[-train_indices, ]

# Prepare data to be suitable for lasso model
train_X <- as.matrix(train2_data[, -1])
test_X <- as.matrix(test2_data[, -1])
train_Y <- factor(train2_data$label, levels = c(-1, 1), labels = c(0, 1))
test_Y <- factor(test2_data$label, levels = c(-1, 1), labels = c(0, 1))

# Verify split
cat("Training set size:", nrow(train2_data), "\n")
cat("Test set size:", nrow(test2_data), "\n")
```

As we are dealing with an extremely high-dimensional dataset, we will perform a feature-pre-selection based on the variance of the features to reduce computational complexity. We will drop all features with a variance below 0.01.

```{r}
# Calculate the variance of each feature
feature_variances <- apply(train_X, 2, var)

# Identify features with variance >= 0.01
selected_features <- which(feature_variances >= 0.01)

# Print the number of features removed
cat("Number of features removed:", ncol(train_X) - length(selected_features), "\n")
cat("Number of features retained:", length(selected_features), "\n")

# Apply on training and test dataset
train_X <- train_X[, selected_features]
test_X <- test_X[, selected_features]
```

The performance metric that we are trying to optimize is the balanced accuracy. To be able to access this metric in our cross-validation when trying to find the best shrinkage parameter lambda, we store the balanced accuracy as a function.

```{r}
# Define custom balanced accuracy function
calculate_balanced_accuracy <- function(data, lev = NULL, model = NULL) {
  confusion_matrix <- table(data$obs, data$pred)
  
  if (nrow(confusion_matrix) < 2 || ncol(confusion_matrix) < 2) {
    sensitivity <- 0
    specificity <- 0
  } else {
    TP <- confusion_matrix[2, 2]
    FN <- confusion_matrix[2, 1]
    TN <- confusion_matrix[1, 1]
    FP <- confusion_matrix[1, 2]
    sensitivity <- TP / (TP + FN)
    specificity <- TN / (TN + FP)  
  }
  
  balanced_acc <- (sensitivity + specificity) / 2
  return(c(BalancedAccuracy = balanced_acc))
}
```

Because of the high imbalance in classes in our dataset, we will assign weights according to the share of the different classes in the dataset and pass these to our model.

```{r}
# Assign weights to classes
class_weights <- ifelse(train2_data$label == 1, nrow(train2_data)/ (2*sum(train2_data$label==1)), nrow(train2_data)/ (2*sum(train2_data$label==-1)))
```

We will perform a 5-fold cross validation on the data using the train control function and balanced accuracy as the metric for evaluation to find the best value for the penalty coefficient lambda.

```{r}
# Define train control with custom function for balanced accuracy
train_control <- trainControl(
  method = "cv",
  number = 10,
  summaryFunction = calculate_balanced_accuracy,
  classProbs = TRUE,
  savePredictions = "final"
)
```

Playing around with different values of lambda showed us, that the optimal value for lambda should lie between 0.00001 and 0.3. Therefore we will define a grid in that range.

```{r}
# Define grid for lambda values
lambda_grid <- expand.grid(
  alpha = 1,
  lambda = 10^seq(-5, -0.5, length.out = 100)
)
```

We will now train our model on the training set and extract the best value for lambda as well as the number of non-zero coefficients, which is equivalent to the number of features that the final Lasso model selects.

```{r}
# Convert label column to factor
train_Y <- factor(train_Y, levels = c(0, 1), labels = c("Class0", "Class1"))
test_Y <- factor(test_Y, levels = c(0, 1), labels = c("Class0", "Class1"))

# Train Lasso model with goal to optimize balanced accuracy
set.seed(42)
lasso_model <- train(
  x = train_X,
  y = train_Y,
  method = "glmnet",
  trControl = train_control,
  tuneGrid = lambda_grid,
  metric = "BalancedAccuracy",
  weights = class_weights
)

# Print the best lambda
best_lambda <- lasso_model$bestTune$lambda
print(lasso_model$bestTune)

# Extract coefficients for the best lambda
coefficients <- coef(lasso_model$finalModel, s = best_lambda)

# Count non-zero coefficients (excluding the intercept)
non_zero_count <- sum(coefficients != 0) - 1
cat("Number of non-zero coefficients:", non_zero_count, "\n")
```

The best value for lambda according to our cross-validation is \~ 0.03898, so this is the value we will use to evaluate our model on the test data. Also, we can see that the number of features selected in our best model is 62. In the following plot we can see how the balanced accuracy differs with respect to the different values that we tested for lambda.

```{r}
# Plot balanced accuracy for training set over range for regularization parameter
plot(lasso_model)
```

The following creates a plot to visualize how the number of selected features depends on the value of lambda chosen for the model.

```{r}
# Extract all lambda values from the model
lambda_values <- lasso_model$finalModel$lambda

# Initialize a vector to store the number of non-zero coefficients per model/lambda
num_features <- numeric(length(lambda_values))

# Loop through each lambda and count non-zero coefficients
for (i in seq_along(lambda_values)) {
  coefficients <- coef(lasso_model$finalModel, s = lambda_values[i])
  num_features[i] <- sum(coefficients != 0) - 1
}

# Plot the results
plot(
  log10(lambda_values), num_features, type = "b",
  xlab = "Log10(Lambda)", ylab = "Number of Selected Features",
  main = "Number of Selected Features vs. Lambda"
)
```

In this final part, we evaluate how well our chosen model (lambda = 0.03898) performs on the unseen test data. The metric of evaluation is still the balanced accuracy.

```{r}
# Predict probabilities on the test data
lasso_probs <- predict(lasso_model, newdata = test_X, type = "prob")

# Convert probabilities to class predictions using a threshold (e.g., 0.5)
lasso_preds <- ifelse(lasso_probs[, "Class1"] > 0.5, "Class1", "Class0")

# Ensure predictions are factors with the same levels as test_Y
lasso_preds <- factor(lasso_preds, levels = levels(test_Y))

# Calculate balanced accuracy
confusion_matrix <- table(test_Y, lasso_preds)
sensitivity <- confusion_matrix[2, 2] / (confusion_matrix[2, 2] + confusion_matrix[2, 1])  # True Positive Rate
specificity <- confusion_matrix[1, 1] / (confusion_matrix[1, 1] + confusion_matrix[1, 2])  # True Negative Rate
sensitivity <- confusion_matrix[2, 2] / (confusion_matrix[2, 2] + confusion_matrix[2, 1])
specificity <- confusion_matrix[1, 1] / (confusion_matrix[1, 1] + confusion_matrix[1, 2])
balanced_accuracy <- (sensitivity + specificity) / 2
cat("Balanced Accuracy on Test Data:", balanced_accuracy, "\n")
```

We get a final balanced accuracy on the test data of 0.76 for the lasso model.

# Random Forest for Feature Selection

We chose Random Forest as the next model for feature selection. We believe that as it builds multiple decision trees and aggregates their results to improve predictive performance while ranking feature importance, it is a robust choice.

At first we create the training and testing set.

```{r}
library(randomForest)
library(caret)

# Random split (80% training, 20% testing)
set.seed(123)
train_indices <- sample(1:nrow(MLData_Task2), size = 0.8 * nrow(MLData_Task2))

# Create training and testing datasets
train2_data <- MLData_Task2[train_indices, ]
test2_data <- MLData_Task2[-train_indices, ]

# Prepare data
train_X <- as.matrix(train2_data[, -1])
test_X <- as.matrix(test2_data[, -1])
train_Y <- factor(train2_data$label, levels = c(-1, 1), labels = c(0, 1))
test_Y <- factor(test2_data$label, levels = c(-1, 1), labels = c(0, 1))

# Verify split
cat("Training set size:", nrow(train2_data), "\n")
cat("Test set size:", nrow(test2_data), "\n")
```

For a random forest of classification trees, we usually use a random selection of m= sqrt(p) predictors as split candidates each time a split in a tree is considered. However in our case, after the pre-selection this would mean m = sqrt(100,000) = \~316, which is extremely computationally expensive. Thus we will apply a pre-selection of features by removing those features with a variance below 0.01 - given the low variance, these features will likely not serve as good predictors for our classification task.

```{r}
# Calculate the variance of each feature
feature_variances <- apply(train_X, 2, var)

# Identify features with variance >= 0.01
selected_features <- which(feature_variances >= 0.01)

# Print the number of features removed
cat("Number of features removed:", ncol(train_X) - length(selected_features), "\n")
cat("Number of features retained:", length(selected_features), "\n")

# Apply on training and test dataset
train_X <- train_X[, selected_features]
test_X <- test_X[, selected_features]
```

sqrt(31,269) still leaves \~173 features to be considered at each split. We will try if the model works for a lower number of features, with the aim to increase interpretability of the final model. Thus we define our grid for the hyperparameter m as follows:

```{r}
# Define the hyperparameter grid for tuning
tune_grid <- expand.grid(
  mtry = c(20, 50, 100)
)
```

We will use the balanced accuracy as our performance metric as the dataset is highly imbalanced.

```{r}
# Write balanced accuracy as a function that can be accessed by trainControl
calculate_balanced_accuracy <- function(data, lev = NULL, model = NULL) {
  confusion_matrix <- table(data$obs, data$pred)
  TP <- confusion_matrix[2, 2]
  FN <- confusion_matrix[2, 1]
  TN <- confusion_matrix[1, 1]
  FP <- confusion_matrix[1, 2]

  sensitivity <- TP / (TP + FN)
  specificity <- TN / (TN + FP)
  balanced_acc <- (sensitivity + specificity) / 2

  return(c(BalancedAccuracy = balanced_acc))
}
```

```{r}
# Assign weights to classes
class_weights <- ifelse(train2_data$label == 1, nrow(train2_data)/ (2*sum(train2_data$label==1)), nrow(train2_data)/ (2*sum(train2_data$label==-1)))
```

We are using a 5-fold Cross-validation to find the optimal split nodes and values.

```{r}
# Set up cross-validation
control <- trainControl(
  method = "cv",
  number = 5, 
  verboseIter = TRUE,
  savePredictions = "final",
  summaryFunction = calculate_balanced_accuracy
)
```

We are training the model on our training dataset that only contains the selected features (variance \> 0.01). We are also evaluating different values for the number of trees and the nodesize in our random forest.

```{r}
# Ensure that the label is stored as a factor
train_Y <- factor(train_Y, levels = c(0, 1), labels = c("Class0", "Class1"))
test_Y <- factor(test_Y, levels = c(0, 1), labels = c("Class0", "Class1"))

# Define values to test for ntree and nodesize
ntree_values <- c(10, 15, 20)
nodesize_values <- c(8, 10, 12)

# Initialize variables to track the best model
best_model <- NULL
best_balanced_accuracy <- 0

# Loop over combinations of ntree and nodesize
for (ntree in ntree_values) {
  for (nodesize in nodesize_values) {
    set.seed(42)
    
    # Train the model for each combination
    rf_tuned_model <- train(
      x = train_X,
      y = train_Y,
      method = "rf",
      metric = "BalancedAccuracy",
      weights = class_weights,
      tuneGrid = tune_grid,  
      trControl = control,
      ntree = ntree,
      nodesize = nodesize,
      importance = TRUE
    )
    
    # Check performance on validation
    best_index <- which.max(rf_tuned_model$results$BalancedAccuracy)
    current_balanced_accuracy <-rf_tuned_model$results$BalancedAccuracy[best_index]
    
    # Update the best model if current is better
    if (current_balanced_accuracy > best_balanced_accuracy) {
      best_balanced_accuracy <- current_balanced_accuracy
      best_model <- rf_tuned_model
    }
  }
}

# Print the best model and performance
print(best_model)
```

The best model has the following combinations of parameter values:

-   mtry = 100

-   ntree = 10

-   nodesize = 10

In random forest, the number of selected features refers to the features that were used in splitting at least one node in the tree. In the following, we will extract the selected features using the importance score that has been assigned to them during the model training.

```{r}
# Extract final model
final_model <- best_model$finalModel

# Use the randomForest importance function
feature_importance <- randomForest::importance(final_model)

# Get features with non-zero importance
selected_features <- rownames(feature_importance)[apply(feature_importance, 1, function(x) any(x > 0))]

# Print selected features
print(length(selected_features))
```

We can see that 205 features have been selected. Now we fit our model on the subset of selected features.

```{r}
# Retrain model on the subset of data with only the selected features 
reduced_train_X <- train2_data[, selected_features]
reduced_test_X <- test2_data[, selected_features]
```

```{r}
# Train best random forest model on the parameters of the final model
rf_final_model <- randomForest(
  x = reduced_train_X,
  y = train_Y,
  ntree = 10,
  mtry = 100, 
  nodesize = 10
)
```

We now evaluate the final model on the unseen test data and extract the balanced accuracy.

```{r}
# Predicting on test set
rf_predictions <- predict(rf_final_model, newdata = test_X)

# Confusion matrix
conf_matrix_rf <- confusionMatrix(rf_predictions, test_Y)

# Display confusion matrix
print(conf_matrix_rf)
```

The balanced accuracy on the testing set is 0.7394.

# Extreme Gradient Boosting (XGBoost)

XGBoost is a gradient boosting algorithm that builds an ensemble of decision trees to optimize predictive accuracy while providing feature importance scores. We chose it as we believe that its scalability and ability to handle high-dimensional data, as well as its speed make it a powerful tool for feature selection.

```{r}
library(xgboost)
```

At first we create the training and testing set.

```{r}
# Random split (80% training, 20% testing)
set.seed(123)
train_indices <- sample(1:nrow(MLData_Task2), size = 0.8 * nrow(MLData_Task2))

# Create training and testing datasets
train2_data <- MLData_Task2[train_indices, ]
test2_data <- MLData_Task2[-train_indices, ]

# Ensure labels are 0 and 1 and stored as numeric vector for XGBoost
train2_data$label <- ifelse(train2_data$label == -1, 0, 1)
test2_data$label <- ifelse(test2_data$label == -1, 0, 1)

# Prepare data for XGBoost
train_matrix <- xgb.DMatrix(data = as.matrix(train2_data[,-1]), label = train2_data$label)
test_matrix <- xgb.DMatrix(data = as.matrix(test2_data[,-1]), label = test2_data$label)
```

We write our function to calculate the balanced accuracy during model fitting and evaluation.

```{r}
balanced_accuracy <- function(preds, dtrain) {
  # Get true labels
  labels <- getinfo(dtrain, "label")
  
  # Convert predicted probabilities to binary classes (threshold = 0.5)
  predictions <- ifelse(preds > 0.5, 1, 0)
  
  # Confusion matrix components
  TP <- sum(predictions == 1 & labels == 1)
  TN <- sum(predictions == 0 & labels == 0)
  FP <- sum(predictions == 1 & labels == 0)
  FN <- sum(predictions == 0 & labels == 1)
  
  # Sensitivity and Specificity
  sensitivity <- TP / (TP + FN)
  specificity <- TN / (TN + FP)
  
  # Balanced Accuracy
  balanced_acc <- (sensitivity + specificity) / 2
  
  # Return metric as a list
  return(list(metric = "BalancedAccuracy", value = balanced_acc))
}
```

Next, we define our gridsearch for the hyperparameter tuning. XGBoost has a lot of different parameters that need to be controlled to optimize the model, which can be separated into three types:

-   **General Parameters:** Define which booster to use, e.g., a tree or a linear model. We will use the default, which is a tree model.

-   **Booster Parameters (tree-specific)**:

    -   **eta**: Specifies the step size shrinkage for the feature weights used in the next iteration of the boosting algorithm. The default is set to 0.3, so we will try one more and one less conservative value to see if it improves performance.

    -   **max_depth**: Specifies the maximum depth of a tree. The default is set to 6, we will try lower and higher values as well.

    -   **colsample_bytree**: Specifies the subsample of columns when constructing each tree. The default is 1, we will try a lower value as well.

-   **Task Parameters**

    -   **base_score**: We will use the default of 0.5

    -   **objective**: Specifies the type of learner. As we have a classification task, we will use logistic regression.

    -   **eval_metric**: Specifies the evaluation metric for validation data. We will use the balanced accuracy, as always.

    -   **seed**: Specifies the seed to reproduce the same set of outputs.

Now we run the cross validation and grid search to identify the best set from the above described booster parameters.

```{r}
# Grid search for hyperparameter tuning
search_grid <- expand.grid(
  max_depth = c(2, 4, 6, 8),
  eta = c(0.01, 0.05, 0.07, 0.1),
  colsample_bytree = c(0.7, 1.0)
)

# Initialize variables to track the best model
best_balanced_acc <- 0
best_params <- list()

# Perform grid search
for (i in 1:nrow(search_grid)) {
  params <- list(
    objective = "binary:logistic",
    max_depth = search_grid$max_depth[i],
    eta = search_grid$eta[i],
    colsample_bytree = search_grid$colsample_bytree[i]
  )
  
  # Perform cross-validation
  cv_results <- xgb.cv(
    params = params,
    data = train_matrix,
    nfold = 5,
    nrounds = 500,
    early_stopping_rounds = 50,
    feval = balanced_accuracy,
    maximize = TRUE,
    verbose = FALSE
  )
  
  # Extract the best Balanced Accuracy
  mean_balanced_acc <- max(cv_results$evaluation_log$test_BalancedAccuracy_mean)
  
  # Update the best model parameters if current model is better
  if (mean_balanced_acc > best_balanced_acc) {
    best_balanced_acc <- mean_balanced_acc
    best_params <- params
    best_nrounds <- cv_results$best_iteration
  }
}
```

```{r}
# Train final model with optimal hyperparameters
final_model <- xgb.train(
  params = best_params,
  data = train_matrix,
  nrounds = best_nrounds
)
```

```{r}
# Print the results
print(best_params)
cat("Best Balanced Accuracy:", best_balanced_acc, "\n")
cat("Best number of rounds:", best_nrounds, "\n")

# Get feature importance from the final model
importance <- xgb.importance(model = final_model)

# Number of features selected (importance score > 0)
selected_features <- nrow(importance)
cat("Number of selected features:", selected_features, "\n")

# Count the number of features with non-zero importance
non_zero_features <- sum(importance$Gain > 0)
cat("Number of features with non-zero importance:", non_zero_features, "\n")
```

The best model has the following parameter values:

-   max_depth - 4 ,

-   eta - 0.1 ,

-   colsample_bytree - 1 .

The best model selects 59 out of the 100,000 features and returns a balanced accuracy of \~0.77 on the training set. As our objective to binary:logistic does not directly predict classes, but rather probabilities, we need to convert the probabilities into our classes.

```{r}
# Use the best model to predict on the test set
xgb_predictions <- predict(final_model, newdata = test_matrix)

binary_predictions <- ifelse(xgb_predictions > 0.5, 1, 0)

# Convert binary predictions to a factor
binary_predictions <- factor(binary_predictions, levels = c(0, 1))

# Convert true labels to a factor with the same levels
true_labels <- factor(test2_data$label, levels = c(0, 1))
```

```{r}
# Store confusion matrix
conf_matrix <- confusionMatrix(binary_predictions, true_labels)

# Print the confusion matrix and balanced accuracy
print(conf_matrix)
```

Our final XGBoost model returns a balanced accuracy of 0.715 on the test data set.

## Elastic Net

Elastic Net is a regularized regression method that combines the strengths of Lasso (L1 regularization) and Ridge (L2 regularization). It is particularly useful for high-dimensional datasets where the number of features exceeds the number of observations or when features are highly correlated. Elastic Net introduces two hyperparameters: alpha, which controls the balance between Lasso and Ridge penalties, and lambda, which determines the overall strength of regularization. By shrinking less important coefficients towards zero and selecting only the most relevant features, Elastic Net reduces model complexity and enhances predictive performance. This makes it a powerful tool for feature selection and classification tasks in challenging datasets.

```{r}
# Load necessary libraries
library(caret)
library(glmnet)
library(doParallel)
```

### Split the dataset

Splits the dataset into 80% training and 20% testing data. This ensures a clear separation between data used for model building and evaluation.

```{r}
set.seed(123)
train_indices <- sample(1:nrow(MLData_Task2), size = 0.8 * nrow(MLData_Task2))
train_data <- MLData_Task2[train_indices, ]
test_data <- MLData_Task2[-train_indices, ]
```

### Extract features and labels:

```{r}
train_X <- as.matrix(train_data[, -1])  
test_X <- as.matrix(test_data[, -1])    
train_Y <- factor(train_data$label, levels = c(-1, 1), labels = c(0, 1))  
test_Y <- factor(test_data$label, levels = c(-1, 1), labels = c(0, 1))    
```

-   train_X and test_X hold feature matrices for training and testing, respectively.
-   train_Y and test_Y store the labels (converted from -1, 1 to 0, 1 for binary classification).

### Feature pre-selection - Variance threshold:

```{r}
feature_variances <- apply(train_X, 2, var)
selected_features <- which(feature_variances >= 0.05)  
train_X <- train_X[, selected_features]
test_X <- test_X[, selected_features]

cat("Number of features after variance filtering:", ncol(train_X), "\n")
```

-   Filters out features with low variance (\< 0.05), as they contribute little predictive power.
-   The threshold 0.05 reduces dimensions to \~1,194 features, balancing computational feasibility and information retention.

# Comment on Threshold:

-   Higher Threshold (0.05): Results in fewer features (\~1k), improving speed and reducing redundancy.
-   Lower Thresholds (0.01, 0.02): Retain more features (up to \~30k), but these cause issues with the Elastic Net method in this implementation (e.g., resulting in NaN balanced accuracy).
-   When thresholds are too low, issues such as multicollinearity and insufficient penalization can arise, causing numerical instability in Elastic Net.

### Scale features:

```{r}
train_X <- scale(train_X)
test_X <- scale(test_X)
```

Standardizes features to have zero mean and unit variance, critical for regularization-based methods like Elastic Net, which are sensitive to feature scales.

### Define class weights

```{r}
class_weights <- ifelse(train_Y == 1, 
                        1 / sum(train_Y == 1), 
                        1 / sum(train_Y == 0))
```

-   Accounts for class imbalance by assigning higher weights to the minority class (class 1).
-   Ensures the model does not favor the majority class disproportionately.

### Custom Balanced Accuracy function

```{r}
calculate_balanced_accuracy <- function(data, lev = NULL, model = NULL) {
  confusion_matrix <- table(data$obs, data$pred)
  if (nrow(confusion_matrix) < 2 || ncol(confusion_matrix) < 2) {
    sensitivity <- 0
    specificity <- 0
  } else {
    TP <- confusion_matrix[2, 2]
    FN <- confusion_matrix[2, 1]
    TN <- confusion_matrix[1, 1]
    FP <- confusion_matrix[1, 2]
    sensitivity <- TP / (TP + FN)
    specificity <- TN / (TN + FP)
  }
  balanced_acc <- (sensitivity + specificity) / 2
  return(c(BalancedAccuracy = balanced_acc))
}
```

-   Calculates the balanced accuracy metric, which is the average of sensitivity (recall for the positive class) and specificity (recall for the negative class).
-   This metric is crucial for imbalanced datasets, providing a fair assessment of model performance across both classes.

###Train Control Setup

```{r}
train_control <- trainControl(
  method = "cv",
  number = 5,
  summaryFunction = calculate_balanced_accuracy,
  savePredictions = "final",
  verboseIter = TRUE
)
```

-   Specifies 5-fold cross-validation for training.
-   Uses the custom calculate_balanced_accuracy function to evaluate model performance during cross-validation.

### Hyperparameter Grid

```{r}
elastic_net_grid <- expand.grid(
  alpha = seq(0, 1, by = 0.2),  
  lambda = 10^seq(-4, -2, length.out = 5)  
)
```

Defines a grid of hyperparameters for Elastic Net: - alpha: Controls the balance between L1 (sparse selection) and L2 (shrinkage) penalties. - lambda: Regularization strength.

### Train Elastic Net Model

```{r}
# Enable parallel processing
cl <- makeCluster(detectCores() - 1)  # Use all available cores except one
registerDoParallel(cl)

# Train Elastic Net model
set.seed(123)
elastic_net_model <- train(
  x = train_X,
  y = train_Y,
  method = "glmnet",
  metric = "BalancedAccuracy",
  trControl = train_control,
  tuneGrid = elastic_net_grid,
  weights = class_weights  # Pass class weights
)

# Stop the cluster after training
stopCluster(cl)

# Print the best parameters
print(elastic_net_model$bestTune)
```

-   Trains the Elastic Net model using the defined hyperparameter grid and class weights.
-   Parallel processing reduces computational time.

### Evaluate Model on Test Set

```{r}
elastic_predictions <- predict(elastic_net_model, test_X)

# Generate confusion matrix
conf_matrix <- confusionMatrix(elastic_predictions, test_Y)

# Print confusion matrix and metrics
print(conf_matrix)
```

Predicts the test set labels and evaluates model performance using a confusion matrix.

### Balanced Accuracy Calculation

```{r}
TP <- conf_matrix$table[2, 2]
FN <- conf_matrix$table[2, 1]
TN <- conf_matrix$table[1, 1]
FP <- conf_matrix$table[1, 2]

sensitivity <- TP / (TP + FN)
specificity <- TN / (TN + FP)
balanced_accuracy <- (sensitivity + specificity) / 2

cat("Balanced Accuracy on Test Data:", balanced_accuracy, "\n")
```

The model achieves a balanced accuracy of 0.83 on the test set, indicating strong overall performance on this imbalanced dataset. The sensitivity (97.89%) is high, showing the model is effective at identifying the majority class. However, specificity (44.44%) is lower, reflecting challenges in correctly classifying the minority class.

Using a variance threshold of 0.05 reduced the features to 1,194, enabling significant dimensionality reduction. Lower thresholds (e.g., 0.01 or 0.02) resulted in NaN balanced accuracy, demonstrating the trade-off between stability and the number of features selected.


### SVM with RFE

RFE (Recursive Feature Elimination) is a feature selection technique that iteratively builds models, removes the least important features, and re-trains the model. Support Vector Machines is an effective classifier in high dimensions and it reduces the risk of overfitting. Together SVM and RFE retains high classification performance by focusing on the most discriminative features while improving the model efficiency.

```{r}
library(caret)
library(e1071)

# Random split (80% training, 20% testing)
set.seed(123)
train_indices <- sample(1:nrow(MLData_Task2), size = 0.8 * nrow(MLData_Task2))

# Create training and testing datasets
train2_data <- MLData_Task2[train_indices, ]
test2_data <- MLData_Task2[-train_indices, ]

# Prepare data
train_X <- as.matrix(train2_data[, -1]) # Exclude label column
test_X <- as.matrix(test2_data[, -1]) # Exclude label column
train_Y <- factor(train2_data$label, levels = c(-1, 1), labels = c(0, 1))
test_Y <- factor(test2_data$label, levels = c(-1, 1), labels = c(0, 1))

# Verify split
cat("Training set size:", nrow(train2_data), "\n")
cat("Test set size:", nrow(test2_data), "\n")
```
Applying a pre-selection of features by removing those features with a variance below 0.01 - given the low variance, these features will likely not serve as good predictors for our classification task.
```{r}
# Calculate the variance of each feature
SVMfeature_variances <- apply(train_X, 2, var)

# Identify features with variance >= 0.01
SVMselected_features <- which(SVMfeature_variances >= 0.01)

# Print the number of features removed
cat("Number of features removed:", ncol(train_X) - length(SVMselected_features), "\n")
cat("Number of features retained:", length(SVMselected_features), "\n")

# Apply on training and test dataset
SVMtrain_X <- train_X[, SVMselected_features]
SVMtest_X <- test_X[, SVMselected_features]


```
Standardize the features by centering (subtracting the mean) and scaling (dividing by the standard deviation). Standardization is important for SVM because it ensures that all features are on the same scale, preventing features with larger magnitudes from dominating the SVM's decision boundary. Centering (subtracting the mean) ensures that each feature has a mean of zero, which helps improve the optimization process and convergence of the SVM algorithm.
```{r}
# Standardize the features by centering and scaling 
preProcValues <- preProcess(SVMtrain_X, method = c("center", "scale"))
SVMtrain_X <- predict(preProcValues, SVMtrain_X)
SVMtest_X <- predict(preProcValues, SVMtest_X)
```

Set up training control for Recursive Feature Elimination (RFE) with cross-validation. The rfeControl function specifies the method of resampling (cross-validation) and the number of folds (10).
Then, run RFE using a SVM with a linear kernel. The rfe function evaluates different subsets of features (sizes 10, 20, 30, 40, 50, 60, 70, 80, 90, 100) to determine the optimal number of features based on model performance. Cross-validation is used to estimate the performance of each feature subset.
```{r}
# Set up training control with cross-validation
ctrl <- rfeControl(functions=rfFuncs, method="cv", number=10)

# Run RFE with SVM model with different sizes to determine the best number of features
svm_rfe <- rfe(SVMtrain_X, train_Y, sizes=c(10, 20, 30, 40, 50, 60, 70, 80, 90, 100), 
               rfeControl=ctrl, method="svmLinear")
print(svm_rfe)
```
Size 30 was selected because it has the highest accuracy at 0.9406


The svm_rfe object contains the result of the RFE process, including the set of features that resulted in the best model performance. Then, subset the training and test data to include only the selected features. This ensures that the model is trained and evaluated only on the most relevant features.
```{r}
# Get the optimal subset of features based on RFE
selected_features <- svm_rfe$optVariables
print(selected_features)

# Subset the training and test data to include only the selected features
X_train_selected <- SVMtrain_X[, selected_features]
X_test_selected <- SVMtest_X[, selected_features]

```
Printed out above are the 30 features that were selected. 

Train the SVM model using the selected features. The model is trained on the selected features (X_train_selected) and the corresponding target labels (train_Y). After training the model, the 'predict' function is used to generate predictions (y_pred) for the test set based on the features (X_test_selected) that were selected during feature selection.
```{r}
# Train the SVM model using the selected features
svm_model <- svm(X_train_selected, as.factor(train_Y), kernel = "linear")

# Generate predictions on the test set using the trained SVM model
y_pred <- predict(svm_model, X_test_selected)

# Ensure y_pred and test_Y are factors with the same levels
y_pred <- factor(y_pred, levels = levels(test_Y)) 
test_Y <- factor(test_Y, levels = levels(y_pred))  
```

Generate the confusion matrix to evaluate the performance of the model and extract the Balanced Accuracy from the confusion matrix results.
```{r}
# Evaluate the model using balanced accuracy
conf_matrix <- confusionMatrix(y_pred, test_Y)
print(conf_matrix)
balanced_accuracy <- conf_matrix$byClass["Balanced Accuracy"]
print(paste("SVM with RFE Balanced Accuracy: ", balanced_accuracy))
```
SVM with RFE Balanced Accuracy: 0.711658841940532

#Gradient Boosting 

Gradient Boosting is a technique that builds models sequentially by combining multiple weak learners, typically decision trees, to create a strong predictive model. Each tree corrects the errors of the previous one by focusing on minimizing a specific loss function (e.g., Mean Squared Error or Log Loss) using gradient descent. This process improves predictions by fitting new models to the residuals of the previous models, making it highly effective for regression and classification tasks.

```{r}
# Loading the required libraries
library(caret)       
library(gbm)         
library(dplyr)       
```

```{r}
# Set seed for reproducibility
set.seed(123)

#Splitting data into training and testing sets (80-20 split)
train_indices <- sample(1:nrow(MLData_Task2), size = 0.8 * nrow(MLData_Task2))
GBtrain_data <- MLData_Task2[train_indices, ]
GBtest_data <- MLData_Task2[-train_indices, ]

# Ensure labels are factors with two levels for classification
GBtrain_data$label <- factor(GBtrain_data$label, levels = c(-1, 1), labels = c("Class1", "Class2"))
GBtest_data$label <- factor(GBtest_data$label, levels = c(-1, 1), labels = c("Class1", "Class2"))
```

```{r}
# Remove non-numeric columns before variance calculation
numeric_features <- sapply(GBtrain_data[, -ncol(GBtrain_data)], is.numeric)  # Exclude label column
GBtrain_data_numeric <- GBtrain_data[, numeric_features]
```

### Feature pre-selection - Variance threshold:

-   Filters out features with low variance (\< 0.05), as they contribute little predictive power.
-   The threshold 0.05 reduces dimensions to \~1,194 features, balancing computational feasibility and information retention.

```{r}
# Calculating variance of numeric features
variance_threshold <- 0.05  
feature_variances <- apply(GBtrain_data_numeric, 2, var, na.rm = TRUE)  

# Select features with variance above the threshold
selected_features <- names(feature_variances[feature_variances > variance_threshold])
cat("Number of features retained after variance threshold:", length(selected_features), "\n")
```

We can see that 1194 features have been selected. Now we will fit our model on the subset of selected features.

```{r}
selected_features <- intersect(selected_features, colnames(GBtrain_data))

# Filtering datasets to include only selected features and the label
GBtrain_data_filtered <- GBtrain_data[, c(selected_features, "label")]
GBtest_data_filtered <- GBtest_data[, c(selected_features, "label")]

# Verifying the filtered dataset dimensions
cat("Filtered training dataset dimensions:", dim(GBtrain_data_filtered), "\n")
cat("Filtered testing dataset dimensions:", dim(GBtest_data_filtered), "\n")
```

### Custom Balanced Accuracy Function

-   Calculates the balanced accuracy metric, which is the average of sensitivity (recall for the positive class) and specificity (recall for the negative class).
-   This metric is crucial for imbalanced datasets, providing a fair assessment of the model performance across both classes.

```{r}
balanced_accuracy <- function(predictions, true_labels) {
  # Confusion matrix components
  TP <- sum(predictions == "Class2" & true_labels == "Class2")
  TN <- sum(predictions == "Class1" & true_labels == "Class1")
  FP <- sum(predictions == "Class2" & true_labels == "Class1")
  FN <- sum(predictions == "Class1" & true_labels == "Class2")
  
  # Sensitivity and Specificity
  sensitivity <- ifelse((TP + FN) > 0, TP / (TP + FN), 0)
  specificity <- ifelse((TN + FP) > 0, TN / (TN + FP), 0)
  
  # Balanced Accuracy
  balanced_acc <- (sensitivity + specificity) / 2
  return(balanced_acc)
}

custom_summary <- function(data, lev = NULL, model = NULL) {
  # Extracting true labels and predictions
  predictions <- data$pred
  true_labels <- data$obs
  
  # Computing the Balanced Accuracy
  balanced_acc <- balanced_accuracy(predictions, true_labels)
  
  
  return(c(BalancedAccuracy = balanced_acc))
}
```

###Train Control Setup

```{r}
# Step 3: Cross-Validation and Tuning
GBtrain_control <- trainControl(
  method = "cv",                    
  number = 5,                       
  summaryFunction = custom_summary, 
  classProbs = FALSE                
)
```

-   Specifies 5-fold cross-validation for training.
-   Uses the custom (custom_summary) function to evaluate model performance during cross-validation.

#Tune grid set up

Defining a grid of hyperparameter combinations for tuning the Model. It specifies various values for key parameters:learning rate, number of trees, maximum tree depth minimum observations required in a terminal node. The result is a comprehensive set of parameter combinations to optimize the model's performance during training.

```{r}
GBtune_grid <- expand.grid(
 shrinkage = c(0.01, 0.03, 0.05, 0.1,0.2),  
  n.trees = 100,     
  interaction.depth = c(3, 5, 7),         
  n.minobsinnode = c(5, 10)           
)
```

#Training the Gradient Booosting Model

```{r}
weights <- ifelse(GBtrain_data_filtered$label == "Class2", 1.5, 1)
gbm_tuned_model <- train(
  label ~ .,
  data = GBtrain_data_filtered,
  method = "gbm",
  weights = weights,
  trControl = GBtrain_control,
  tuneGrid = GBtune_grid,
  metric = "BalancedAccuracy",
  verbose = FALSE
)

# Printing the best model parameters
cat("Best Model Parameters:\n")
print(gbm_tuned_model$bestTune)
```

-Training the model on the filtered data with weights to handle class imbalance, prioritizing "Class2" by assigning it a higher weight of 1.5.

-It optimizes the model using a grid of hyperparameters and evaluates performance based on balanced accuracy, then outputs the best hyperparameter combination.

#Evaluating on the test data

```{r}
# Predict class labels
test_class_predictions <- predict(gbm_tuned_model, newdata = GBtest_data_filtered)

# Confusion Matrix
conf_matrix <- confusionMatrix(test_class_predictions, GBtest_data_filtered$label)
cat("Confusion Matrix:\n")
print(conf_matrix)
```

Predicting the test set labels and evaluating the model performance using a confusion matrix

# Computing the Balanced Accuracy on test data

```{r}
test_balanced_acc <- balanced_accuracy(test_class_predictions, GBtest_data_filtered$label)
cat("Test Balanced Accuracy:", test_balanced_acc, "\n")
```

The model achieves a balanced accuracy of 0.6944 on the test set, indicating moderate overall performance on the imbalanced dataset. The sensitivity is high, showing effectiveness in identifying the majority class, while specificity is lower, reflecting challenges in correctly classifying the minority class.
